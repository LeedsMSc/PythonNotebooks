{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe571fa0-523c-488f-9db7-0127321ed1a5",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaa1ca6-6183-4d3e-84a9-ecaaef4c5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import easygui \n",
    "import nbimporter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "import xlwings as xw\n",
    "import unidecode\n",
    "\n",
    "from thefuzz import fuzz\n",
    "from openpyxl.utils import column_index_from_string\n",
    "from openpyxl.utils import get_column_letter\n",
    "from pathlib import Path\n",
    "from findfiles import FindFiles \n",
    "from datatransfer import ImportData, ExportData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bc258",
   "metadata": {},
   "source": [
    "# Search for Excel File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17ad67-0dc3-4a6e-a4f1-ddcd131eb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for files and output as text file (uncomment line below)\n",
    "# find_files = FindFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbc696-7450-4fb9-ab74-0dbd564fbd94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deduplication DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a2efb8-f980-4663-ae3d-0b75edb69861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel files imported:\n",
      "\n",
      "5. Cinhal.xlsx                              47 rows    6 Columns\n",
      "5. Embase.xlsx                             749 rows    6 Columns\n",
      "5. Medline.xlsx                            311 rows    6 Columns\n",
      "5. Scopus.xlsx                             226 rows    6 Columns\n",
      "5. Web of Science.xlsx                     179 rows    6 Columns\n"
     ]
    }
   ],
   "source": [
    "# Files and absoloute paths for database searchs \n",
    "data_files_1 = [\n",
    "    Path.home().joinpath(*[\"Desktop\",\"ScopingData\",\"5. Cinhal.xlsx\" ]),\n",
    "    Path.home().joinpath(*[\"Desktop\",\"ScopingData\",\"5. Embase.xlsx\" ]),\n",
    "    Path.home().joinpath(*[\"Desktop\",\"ScopingData\",\"5. Medline.xlsx\" ]),\n",
    "    Path.home().joinpath(*[\"Desktop\",\"ScopingData\",\"5. Scopus.xlsx\" ]),\n",
    "    Path.home().joinpath(*[\"Desktop\",\"ScopingData\",\"5. Web of Science.xlsx\" ]),\n",
    "]\n",
    "\n",
    "# Instantiate ImportData class\n",
    "import_data_1 = ImportData(files_for_import = data_files_1, \\\n",
    "                         worksheet_index = 1, \\\n",
    "                         headings_data_row = 1, \\\n",
    "                         index_column = None, \\\n",
    "                         right_data_column = \"L\",\n",
    "                         has_spacing_columns = True,\n",
    "                         column_to_copy = False)\n",
    "\n",
    "# Create a dataframe\n",
    "import_data_1.make_dataframes()\n",
    "dataframe_1 = import_data_1.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431ce32-5908-4bbb-a122-8ca2eee6da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel files imported:\n",
      "\n",
      "5. References Screen.xlsx                    2 rows    6 Columns\n"
     ]
    }
   ],
   "source": [
    "# Files and absoloute paths for references screen \n",
    "data_files_2 = [\n",
    "    Path.home().joinpath(*[\"OneDrive\",\"Desktop\",\"Scoping Review\",\"5. References Screen.xlsx\" ]), \\\n",
    "             ]\n",
    "\n",
    "# Instantiate ImportData class\n",
    "import_data_2 = ImportData(files_for_import = data_files_2, \\\n",
    "                         worksheet_index = 1, \\\n",
    "                         headings_data_row = 1, \\\n",
    "                         index_column = None, \\\n",
    "                         right_data_column = \"L\",\n",
    "                         has_spacing_columns = True,\n",
    "                         column_to_copy = False)\n",
    "\n",
    "# Create a dataframe\n",
    "import_data_2.make_dataframes()\n",
    "dataframe_2 = import_data_2.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25d070-9f1b-4cf7-ad94-6bb01e53f4c4",
   "metadata": {},
   "source": [
    "# DataFrame Feltling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68525c-d331-41d2-b9fa-f7f4fe8b5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fettling:\n",
    "    def __init__(self, df, name):\n",
    "        self.df = df\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "    def add_key_column(self):\n",
    "        if 'Key' in self.df.columns:\n",
    "            return None\n",
    "        self.df = self.df.reset_index().rename(columns={'index': 'Key'})\n",
    "\n",
    "        \n",
    "    def double_to_int(self):\n",
    "        self.df = self.df.astype({'Year': pd.Int64Dtype(), 'Key': pd.Int64Dtype()})\n",
    "\n",
    "        \n",
    "    def key_as_first_column(self):\n",
    "        self.df = self.df[['Key',\n",
    "                           'Author',\n",
    "                           'Title',\n",
    "                           'Year',\n",
    "                           'Journal',\n",
    "                           'Name of Database',\n",
    "                           'Abstract']].copy()\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        self.add_key_column()\n",
    "        self.double_to_int()\n",
    "        self.key_as_first_column()\n",
    "        export_data = ExportData(self.df, f\"{self.name} {self.df.shape}\")\n",
    "        export_data.export_to_excel()\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fettling_1 = Fettling(dataframe_1, \"database_results\")\n",
    "    dataframe_1 = fettling_1.run()\n",
    "    fettling_2 = Fettling(dataframe_2, \"references screen\")\n",
    "    dataframe_2 = fettling_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439c37e",
   "metadata": {},
   "source": [
    "# Merge The Search DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb82b2-7d8c-4a19-a02b-97141ce552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2.index = dataframe_2.index + 9000\n",
    "dataframe_2[\"Key\"] = dataframe_2.index\n",
    "dataframe = pd.concat([dataframe_1, dataframe_2], axis=\"rows\", ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6982-9a72-4da8-bd37-d14671f215d2",
   "metadata": {},
   "source": [
    "# Add The Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0072b19-4d69-44f8-9594-318afdef096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel files imported:\n",
      "\n",
      "5. Missing Updated.xlsx                     10 rows    6 Columns\n"
     ]
    }
   ],
   "source": [
    "# Files and absoloute paths\n",
    "data_files_3 = [\n",
    "    Path.home().joinpath(*[\"OneDrive\",\"Desktop\",\"Scoping Review\",\"5. Missing Updated.xlsx\" ]), \\\n",
    "            ]\n",
    "\n",
    "# Instantiate ImportData class\n",
    "import_data_3 = ImportData(files_for_import = data_files_3, \\\n",
    "                            worksheet_index = 1, \\\n",
    "                            headings_data_row = 1, \\\n",
    "                            index_column = False, \\\n",
    "                            right_data_column = \"N\",\n",
    "                            has_spacing_columns = True,\n",
    "                            column_to_copy = \"Key\")\n",
    "\n",
    "# Create a dataframe\n",
    "import_data_3.make_dataframes()\n",
    "df_missing_updated = import_data_3.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d56af-f3c9-4ec9-b609-027c1315a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateMissing:\n",
    "    def __init__(self, primary_dataframe, df_missing_updated):\n",
    "        self.primary_dataframe = primary_dataframe\n",
    "        self.df_missing_updated = df_missing_updated\n",
    "\n",
    "        \n",
    "    def missing_values(self):\n",
    "        df_missing_values = self.primary_dataframe[self.primary_dataframe[[\"Author\", \"Title\", \"Year\", \"Journal\"]] \\\n",
    "                                                .isna().any(axis=\"columns\")] \\\n",
    "                                                [[\"Key\", \"Author\", \"Title\", \"Year\", \"Journal\", \"Name of Database\"]]\n",
    "        \n",
    "        export_data = ExportData(df_missing_values, f\"missing_values {df_missing_values.shape}\")\n",
    "        export_data.export_to_excel()\n",
    "        \n",
    "    \n",
    "    def missing_added(self):\n",
    "        export_data = ExportData(self.df_missing_updated, f\"missing_added {self.df_missing_updated.shape}\")\n",
    "        export_data.export_to_excel()\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.missing_values()\n",
    "        self.missing_added()\n",
    "        self.primary_dataframe.update(self.df_missing_updated)\n",
    "        return self.primary_dataframe\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    update_missing = UpdateMissing(dataframe, df_missing_updated)\n",
    "    dataframe = update_missing.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e706b79-98db-410a-b32e-1e83de6d521f",
   "metadata": {},
   "source": [
    "# Dedeuplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2839cc7-cbc5-45af-bcdb-823e1102b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialDeduplication:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    def new_columns(self):\n",
    "        # Replace the accented characters with non-accented characters\n",
    "        self.df['Title'] = self.df['Title'].apply(lambda x: unidecode.unidecode(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        \n",
    "        # Create a first author column\n",
    "        if not 'First Author' in self.df.columns: \n",
    "            self.df.insert(2, 'First Author', self.df['Author'] \\\n",
    "                                                    .str.split().str[0] \\\n",
    "                                                    .str.lower() \\\n",
    "                                                    .str.strip(\",\") \\\n",
    "                                                    .str.replace(\"ć\", \"c\", regex=False))\n",
    "\n",
    "        # Create a title lower column\n",
    "        if not 'Title Lower' in self.df.columns:\n",
    "            self.df.insert(4, 'Title Lower', self.df[\"Title\"]\n",
    "                                                .str.replace(r'\\d+', '', regex=True)\n",
    "                                                .str.replace(\"+\", \" \", regex=False)\n",
    "                                                .str.replace(\"]\", \" \", regex=False)\n",
    "                                                .str.replace(\"[\", \" \", regex=False)\n",
    "                                                .str.replace(\":\", \" \", regex=False)\n",
    "                                                .str.replace(\"-\", \" \", regex=False)\n",
    "                                                .str.replace(\"()\", \" \", regex=False)\n",
    "                                                .str.replace(\",\", \" \", regex=False)\n",
    "                                                .str.replace(r'\\s+', ' ', regex=True)\n",
    "                                                .str.replace(\"hematology\", \"haematology\")\n",
    "                                                .str.lower()\n",
    "                                                .str.strip())\n",
    "\n",
    "        \n",
    "    def originals_and_duplicates_dataframes(self):  \n",
    "        # Make orignals duplicates dataframe and updatae\n",
    "        df_originals = self.df[~self.df.duplicated(subset=[\"Title Lower\", \"First Author\"], keep=False)] \\\n",
    "                           .sort_values(by=[\"Title Lower\"], ascending=[True]) \\\n",
    "                           [[\"Key\", \"Title\", \"Title Lower\", \"Author\", \"First Author\",\"Year\", \"Journal\", \"Name of Database\", \"Abstract\"]]\n",
    "\n",
    "        df_duplicates = self.df [self.df.duplicated(subset=[\"Title Lower\", \"First Author\"], keep=False)] \\\n",
    "                            .sort_values(by=[\"Title Lower\"], ascending=[True]) \\\n",
    "                            [[\"Key\", \"Title\", \"Title Lower\", \"Author\", \"First Author\",\"Year\", \"Journal\", \"Name of Database\", \"Abstract\"]]\n",
    "       \n",
    "        # Export dataframes\n",
    "        export_data_1 = ExportData(df_originals, f\"originals {df_originals.shape}\")\n",
    "        export_data_2 = ExportData(df_duplicates, f\"duplicates {df_duplicates.shape}\")\n",
    "        export_data_1.export_to_excel()\n",
    "        export_data_2.export_to_excel()\n",
    "    \n",
    "    \n",
    "    def export_dataframe(self):\n",
    "        self.df = self.df.drop_duplicates(subset=[\"Title Lower\", \"First Author\"], keep=\"first\") \\\n",
    "                      .sort_values(by=[\"Key\"], ascending=[True]) \\\n",
    "                      [[\"Key\", \"Title\", \"Title Lower\", \"Author\", \"First Author\", \"Year\", \"Journal\", \"Name of Database\", \"Abstract\"]] \\\n",
    "                      .copy()   \n",
    "        export_data_3 = ExportData(self.df, f\"initial_deduplication {self.df.shape}\")\n",
    "        export_data_3.export_to_excel()\n",
    "        \n",
    "        \n",
    "    def run(self): \n",
    "        self.new_columns()        \n",
    "        self.originals_and_duplicates_dataframes()\n",
    "        self.export_dataframe()\n",
    "        return self.df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    initial_deduplication = InitialDeduplication(dataframe)\n",
    "    dataframe = initial_deduplication.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3215faf-27a5-451b-ba59-37c6fa6b090a",
   "metadata": {},
   "source": [
    "# Similarity Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df4491-ac8d-4a78-ab9f-e7e2fb1a30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDuplicates:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.flag_idx = 1\n",
    "        self.df[\"Query Duplicate\"] = False\n",
    "        self.df[\"Flag Number\"] = np.nan\n",
    "\n",
    "        \n",
    "    def similarity_check(self, target, other):\n",
    "        # Find document titles with > 90% similarity\n",
    "        if isinstance(target[1], float) or isinstance(other[1], float): return None\n",
    "        if target[1] == other[1]: return None\n",
    "        if self.df.loc[target[0], \"Query Duplicate\"] == True: return None\n",
    "        match_score = fuzz.ratio(target[1], other[1])\n",
    "\n",
    "        # Add flag if match score > 90\n",
    "        if match_score >= 90: \n",
    "            self.df.loc[[target[0], other[0]], \"Query Duplicate\"] = True\n",
    "            self.df.loc[[target[0], other[0]], \"Flag Number\"] = self.flag_idx\n",
    "            self.flag_idx += 1\n",
    "\n",
    "            \n",
    "    def title_lower_query(self):\n",
    "        for target_index, target_value in self.df[\"Title Lower\"].iteritems():\n",
    "            for other_index, other_value in self.df[\"Title Lower\"].iteritems():\n",
    "                self.similarity_check((target_index,target_value), (other_index, other_value)) \n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.title_lower_query()\n",
    "        filt = self.df[\"Query Duplicate\"] == True\n",
    "        df_query_duplicates = self.df[filt].sort_values(by=\"Flag Number\")    \n",
    "        export_data = ExportData(df_query_duplicates, f\"query_duplicates {df_query_duplicates.shape}\")\n",
    "        export_data.export_to_excel()\n",
    "        return self.df\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    query_duplicates = QueryDuplicates(dataframe)\n",
    "    dataframe =  query_duplicates.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa161e5-ef60-401e-8fac-ff1e5271199b",
   "metadata": {},
   "source": [
    "# Possible Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246824b2-67cd-4dd0-85ce-90f630bbe355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make query duplicate groups \n",
    "duplicates_dataframe_list = [pd.DataFrame(group) for _, group in dataframe.groupby(\"Flag Number\")] \n",
    "duplicates_dataframe_list = [df.sort_values(by=\"Title Lower\", ascending=True).reset_index(drop=True) for df in duplicates_dataframe_list]\n",
    "\n",
    "# Make each duplicate group into a dataframe\n",
    "for idx in range(0, len(duplicates_dataframe_list), 1):\n",
    "    sheet_name = (f'{int(duplicates_dataframe_list[idx].loc[0, \"Key\"])}_{duplicates_dataframe_list[idx].loc[0, \"First Author\"]}')             \n",
    "    \n",
    "    # Export dataframes\n",
    "    export_data = ExportData(duplicates_dataframe_list[idx], f\"{sheet_name}\")\n",
    "    export_data.export_to_excel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0123f4-f8ad-4aff-87cf-4b81662e1b17",
   "metadata": {},
   "source": [
    "# Final Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093efd52-e976-4705-821e-2779b5e15494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalDeduplication:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        \n",
    "    def edit_flag_numbers(self):\n",
    "        self.df.loc[32, \"Flag Number\"] = 23\n",
    "        self.df.loc[38, \"Flag Number\"] = np.nan\n",
    "        self.df.loc[627, \"Flag Number\"] = np.nan\n",
    "        self.df.loc[265, \"Flag Number\"] = np.nan\n",
    "        self.df.loc[152, \"Flag Number\"] = np.nan\n",
    "\n",
    "        \n",
    "    def duplicate_on_flag_numbers(self):\n",
    "        non_flagged = self.df[self.df[\"Flag Number\"].isna()]\n",
    "        flagged = self.df[~self.df[\"Flag Number\"].isna()]\n",
    "        \n",
    "        flagged = flagged.drop_duplicates(subset=[\"Flag Number\"], keep=\"first\") \\\n",
    "                         .sort_values(by=[\"First Author\"], ascending=[True]) \\\n",
    "                         [[\"Key\", \"Title\", \"Title Lower\", \"Author\", \"First Author\", \"Year\", \"Journal\", \"Name of Database\", \"Abstract\"]]\n",
    "\n",
    "        self.df = pd.concat([non_flagged, flagged], ignore_index=False).sort_values(by=\"Year\",ascending=True) \\\n",
    "                            [[\"Key\", \"Title\", \"Title Lower\", \"Author\", \"First Author\", \"Year\", \"Journal\", \"Name of Database\", \"Abstract\"]] \\\n",
    "                            .astype({'Year': pd.Int64Dtype(), 'Key': pd.Int64Dtype()})\n",
    "    \n",
    "    \n",
    "    def export_dataframe(self):\n",
    "        export_data = ExportData(self.df, f\"final_deduplication {self.df.shape}\")\n",
    "        export_data.export_to_excel()\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        self.edit_flag_numbers()\n",
    "        self.duplicate_on_flag_numbers()\n",
    "        self.export_dataframe()\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    final_deduplication = FinalDeduplication(dataframe)\n",
    "    dataframe =  final_deduplication.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
